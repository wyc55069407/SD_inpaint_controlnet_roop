{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from pathlib import Path\nimport torch\nimport argparse\nfrom torch.onnx import _export as torch_onnx_export\nfrom openvino.tools.mo import convert_model\nfrom openvino.runtime import serialize\nfrom diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel\nfrom diffusers.utils import load_image\nimport numpy as np\n\ndef parse_args() -> argparse.Namespace:\n    \"\"\"Parse and return command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(add_help=False)\n    args = parser.add_argument_group('Options')\n    # fmt: off\n    args.add_argument('-h', '--help', action = 'help',\n                      help='Show this help message and exit.')\n    args.add_argument('-b', '--batch', type = int, default = 1, required = True,\n                      help='Required. batch_size for solving single/multiple prompt->image generation.')\n    args.add_argument('-sd','--sd_weights', type = str, default=\"\", required = True,\n                      help='Specify the path of stable diffusion model')\n    args.add_argument('-lt','--lora_type', type = str, default=\"\", required = False,\n                      help='Specify the type of lora weights, you can choose \"safetensors\" or \"bin\"')\n    args.add_argument('-lw', '--lora_weights', type = str, default=\"\", required = False,\n                      help='Add lora weights to Stable diffusion.')\n    # fmt: on\n    return parser.parse_args()\n\nargs = parse_args()\n###covnert controlnet to IR\ncontrolnet = ControlNetModel.from_pretrained(\"control_v11p_sd15_inpaint\", torch_dtype=torch.float32)\ninputs = {\n    \"sample\": torch.randn((args.batch*2, 4, 64, 64)), \n    \"timestep\": torch.tensor(1),\n    \"encoder_hidden_states\": torch.randn((args.batch*2,77,768)),\n    \"controlnet_cond\": torch.randn((args.batch*2,3,512,512)) #batch=2\n}\n'''dynamic_names = {\n    \"sample\": {0: \"batch\"},\n    \"encoder_hidden_states\": {0: \"batch\", 1: \"sequence\"},\n    \"controlnet_cond\": {0: \"batch\"},\n}'''\n\nCONTROLNET_ONNX_PATH = Path('controlnet-inpaint.onnx')\nCONTROLNET_OV_PATH = CONTROLNET_ONNX_PATH.with_suffix('.xml')\ncontrolnet.eval()\nwith torch.no_grad():\n    down_block_res_samples, mid_block_res_sample = controlnet(**inputs, return_dict=False)\n\ncontrolnet_output_names = [f\"down_block_res_sample_{i}\" for i in range(len(down_block_res_samples))]\ncontrolnet_output_names.append(\"mid_block_res_sample\")\n\nif not CONTROLNET_OV_PATH.exists():\n    if not CONTROLNET_ONNX_PATH.exists():\n\n        with torch.no_grad():\n            torch_onnx_export(controlnet, inputs, CONTROLNET_ONNX_PATH, input_names=list(inputs),\n                output_names=controlnet_output_names,onnx_shape_inference=False, #dynamic_axes=dynamic_names,\n                operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)\n\n    ov_ctrlnet = convert_model(CONTROLNET_ONNX_PATH, compress_to_fp16=True)\n    serialize(ov_ctrlnet,CONTROLNET_OV_PATH)\n    del ov_ctrlnet\n    print('ControlNet successfully converted to IR')\nelse:\n    print(f\"ControlNet will be loaded from {CONTROLNET_OV_PATH}\")\n\n\n###convert SD-Unet model to IR\npipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(args.sd_weights, controlnet=controlnet)\nif args.lora_type == \"bin\":\n    pipe.unet.load_attn_procs(args.lora_weights)\nelif args.lora_type == \"safetensors\":\n    print(\"==make sure you already generate new SD model with lora by diffusers.scripts.convert_lora_safetensor_to_diffusers.py==\")\nelse:\n    print(\"==No lora==\")\nUNET_ONNX_PATH = Path('unet_controlnet/unet_controlnet.onnx')\nUNET_OV_PATH = UNET_ONNX_PATH.parents[1] / 'unet_controlnet.xml'\n\nif not UNET_OV_PATH.exists():\n    if not UNET_ONNX_PATH.exists():\n        UNET_ONNX_PATH.parent.mkdir(exist_ok=True)\n        inputs.pop(\"controlnet_cond\", None)\n        inputs[\"down_block_additional_residuals\"] = down_block_res_samples\n        inputs[\"mid_block_additional_residual\"] = mid_block_res_sample\n\n        unet = pipe.unet\n        unet.eval()\n\n        input_names = [\"sample\", \"timestep\", \"encoder_hidden_states\", *controlnet_output_names]\n        '''dynamic_names = {\n            \"sample\": {0: \"batch\"},\n            \"encoder_hidden_states\": {0: \"batch\", 1: \"sequence\"},\n            \"controlnet_cond\": {0: \"batch\"},\n        }'''\n\n        with torch.no_grad():\n            torch_onnx_export(unet, inputs, str(UNET_ONNX_PATH), #dynamic_axes=dynamic_names,\n                input_names=input_names, output_names=[\"sample_out\"], onnx_shape_inference=False, opset_version=15)\n        del unet\n    del pipe.unet\n    ov_unet = convert_model(UNET_ONNX_PATH, compress_to_fp16=True)\n    serialize(ov_unet,UNET_OV_PATH)\n    del ov_unet\n    print('Unet successfully converted to IR')\nelse:\n    del pipe.unet\n    print(f\"Unet will be loaded from {UNET_OV_PATH}\")\n\n###convert SD-text_encoder model to IR\nTEXT_ENCODER_ONNX_PATH = Path('text_encoder.onnx')\nTEXT_ENCODER_OV_PATH = TEXT_ENCODER_ONNX_PATH.with_suffix('.xml')\n\ndef convert_encoder_onnx(text_encoder:torch.nn.Module, onnx_path:Path):\n    if not onnx_path.exists():\n        input_ids = torch.ones((args.batch, 77), dtype=torch.long)\n        # switch model to inference mode\n        text_encoder.eval()\n\n        # disable gradients calculation for reducing memory consumption\n        with torch.no_grad():\n            # infer model, just to make sure that it works\n            text_encoder(input_ids)\n            # export model to ONNX format\n            torch_onnx_export(\n                text_encoder,  # model instance\n                input_ids,  # inputs for model tracing\n                onnx_path,  # output file for saving result\n                input_names=['tokens'],  # model input name for onnx representation\n                output_names=['last_hidden_state', 'pooler_out'],  # model output names for onnx representation\n                opset_version=14,  # onnx opset version for export\n                onnx_shape_inference=False\n            )\n        print('Text Encoder successfully converted to ONNX')\n\nif not TEXT_ENCODER_OV_PATH.exists():\n    convert_encoder_onnx(pipe.text_encoder, TEXT_ENCODER_ONNX_PATH)\n    ov_txten = convert_model(TEXT_ENCODER_ONNX_PATH, compress_to_fp16=True)\n    serialize(ov_txten,TEXT_ENCODER_OV_PATH)\n    print('Text Encoder successfully converted to IR')\nelse:\n    print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV_PATH}\")\n\n\n###convert VAE model to IR\nVAE_DECODER_ONNX_PATH = Path('vae_decoder.onnx')\nVAE_DECODER_OV_PATH = VAE_DECODER_ONNX_PATH.with_suffix('.xml')\n\ndef convert_vae_decoder_onnx(vae: torch.nn.Module, onnx_path: Path):\n    \"\"\"\n    Convert VAE model to ONNX, then IR format. \n    Function accepts pipeline, creates wrapper class for export only necessary for inference part, \n    prepares example inputs for ONNX conversion via torch.export, \n    Parameters: \n        vae (torch.nn.Module): VAE model\n        onnx_path (Path): File for storing onnx model\n    Returns:\n        None\n    \"\"\"\n    class VAEDecoderWrapper(torch.nn.Module):\n        def __init__(self, vae):\n            super().__init__()\n            self.vae = vae\n\n        def forward(self, latents):\n            return self.vae.decode(latents)\n\n    if not onnx_path.exists():\n        vae_decoder = VAEDecoderWrapper(vae)\n        latents = torch.zeros((args.batch, 4, 64, 64))\n\n        vae_decoder.eval()\n        with torch.no_grad():\n            torch.onnx.export(vae_decoder, latents, onnx_path, input_names=[\n                              'latents'], output_names=['sample'])\n        print('VAE decoder successfully converted to ONNX')\n\n\nif not VAE_DECODER_OV_PATH.exists():\n    convert_vae_decoder_onnx(pipe.vae, VAE_DECODER_ONNX_PATH)\n    ov_vae = convert_model(VAE_DECODER_ONNX_PATH, compress_to_fp16=True)\n    serialize(ov_vae,VAE_DECODER_OV_PATH)\n    print('VAE decoder successfully converted to IR')\nelse:\n    print(f\"VAE decoder will be loaded from {VAE_DECODER_OV_PATH}\")\n    \nVAE_ENCODER_ONNX_PATH = Path('vae_encoder.onnx')\nVAE_ENCODER_OV_PATH = VAE_ENCODER_ONNX_PATH.with_suffix('.xml')\n\ndef convert_vae_encoder_onnx(vae: torch.nn.Module, onnx_path: Path):\n\n    class VAEEncoderWrapper(torch.nn.Module):\n        def __init__(self, vae):\n          super().__init__()\n          self.vae = vae\n        \n        def forward(self, image):\n          return self.vae.encode(image).latent_dist.sample()\n    \n    if not onnx_path.exists():\n    \n        vae_encoder = VAEEncoderWrapper(vae)\n        image = torch.zeros((1, 3, 512, 512))\n        \n        vae_encoder.eval()\n        \n        with torch.no_grad():\n          torch.onnx.export(vae_encoder, image, onnx_path, \n            input_names=['sample'], \n            output_names=['latents'])\n        \n        print('VAE encoder successfully converted to ONNX')\n    \nif not VAE_ENCODER_OV_PATH.exists():\n    convert_vae_encoder_onnx(pipe.vae, VAE_ENCODER_ONNX_PATH)\n    ov_vae = convert_model(VAE_ENCODER_ONNX_PATH)\n    serialize(ov_vae,VAE_ENCODER_OV_PATH)\n    print('VAE encoder successfully converted to IR')\nelse:\n    print(f\"VAE encoder will be loaded from {VAE_ENCODER_OV_PATH}\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from PIL import Image\nfrom diffusers import UniPCMultistepScheduler, EulerAncestralDiscreteScheduler, StableDiffusionControlNetInpaintPipeline, ControlNetModel\nimport torch\nimport numpy as np\nimport argparse\nfrom typing import Union, List, Optional, Tuple\nfrom diffusers.utils import load_image\nfrom diffusers.pipeline_utils import DiffusionPipeline\nfrom transformers import CLIPTokenizer\nfrom openvino.runtime import Core, Model, Type\nfrom openvino.runtime.passes import Manager, GraphRewrite, MatcherPass, WrapType, Matcher\nfrom openvino.runtime import opset10 as ops\nfrom safetensors.torch import load_file\nimport time\nimport cv2\nfrom diffusers.image_processor import VaeImageProcessor\nimport diffusers.image_processor\n\ndef parse_args() -> argparse.Namespace:\n    \"\"\"Parse and return command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(add_help=False)\n    args = parser.add_argument_group('Options')\n    # fmt: off\n    args.add_argument('-h', '--help', action = 'help',\n                      help='Show this help message and exit.')\n    args.add_argument('-lp', '--lora_path', type = str, default = \"\", required = False,\n                      help='Specify path of lora weights *.safetensors')\n    args.add_argument('-a','--alpha',type = float, default = 0.75, required = False,\n                      help='Specify the merging ratio of lora weights, default is 0.75.')\n    args.add_argument('-lp2', '--lora_path2', type = str, default = \"\", required = False,\n                      help='Specify path of lora weights *.safetensors')\n    args.add_argument('-a2','--alpha2',type = float, default = 0.75, required = False,\n                      help='Specify the merging ratio of lora weights, default is 0.75.')                  \n    return parser.parse_args()\n\ndef scale_fit_to_window(dst_width:int, dst_height:int, image_width:int, image_height:int):\n    im_scale = min(dst_height / image_height, dst_width / image_width)\n    return int(im_scale * image_width), int(im_scale * image_height)\n\ndef preprocess(image: Image.Image):\n    src_width, src_height = image.size\n    dst_width, dst_height = scale_fit_to_window(512, 512, src_width, src_height)\n    image = np.array(image.resize((dst_width, dst_height), resample=Image.Resampling.LANCZOS))[None, :]\n    pad_width = 512 - dst_width\n    pad_height = 512 - dst_height\n    pad = ((0, 0), (0, pad_height), (0, pad_width), (0, 0))\n    image = np.pad(image, pad, mode=\"constant\")\n    #image = np.squeeze(image)\n    #image = cv2.copyMakeBorder(image, int(pad_height//2), 512-int(pad_height//2)-dst_height, int(pad_width//2), 512-int(pad_width//2)-dst_width, cv2.BORDER_CONSTANT, (0,0,0) );\n    #cv2.imwrite(\"preprocess.png\",image)\n    #image = np.expand_dims(image, axis=0)\n    image = image.astype(np.float32) / 255.0\n    image = image.transpose(0, 3, 1, 2)\n    return image, pad\n\n\ndef randn_tensor(\n    shape: Union[Tuple, List],\n    dtype: Optional[np.dtype] = np.float32,\n):\n    latents = np.random.randn(*shape).astype(dtype)\n\n    return latents\n\nclass InsertLoRA(MatcherPass):\n    def __init__(self,lora_dict_list):\n        MatcherPass.__init__(self)\n        self.model_changed = False\n\n        param = WrapType(\"opset10.Convert\")\n\n        def callback(matcher: Matcher) -> bool:\n            root = matcher.get_match_root()\n            root_output = matcher.get_match_value()\n            for y in lora_dict_list:\n                if root.get_friendly_name().replace('.','_').replace('_weight','') == y[\"name\"]:\n                    consumers = root_output.get_target_inputs()\n                    lora_weights = ops.constant(y[\"value\"],Type.f32,name=y[\"name\"])\n                    add_lora = ops.add(root,lora_weights,auto_broadcast='numpy')\n                    for consumer in consumers:\n                        consumer.replace_source_output(add_lora.output(0))\n\n                    # For testing purpose\n                    self.model_changed = True\n                    # Use new operation for additional matching\n                    self.register_new_node(add_lora)\n\n            # Root node wasn't replaced or changed\n            return False\n\n        self.register_matcher(Matcher(param,\"InsertLoRA\"), callback)\n\ng_device = None\ng_vae = None\n\nclass OVContrlNetStableDiffusionPipeline(DiffusionPipeline):\n    \"\"\"\n    OpenVINO inference pipeline for Stable Diffusion with ControlNet guidence\n    \"\"\"\n    def __init__(\n        self,\n        tokenizer: CLIPTokenizer,\n        scheduler,\n        core: Core,\n        controlnet: Model,\n        text_encoder: Model,\n        unet: Model,\n        vae_decoder: Model,\n        vae_encoder: Model,\n        state_dict,\n        alpha_list,\n        device:str = \"AUTO\"\n    ):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.vae_scale_factor = 8 #2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.scheduler = scheduler\n        self.load_models(core, device, controlnet, text_encoder, unet, vae_decoder, vae_encoder, state_dict, alpha_list)\n        self.set_progress_bar_config(disable=True)\n        g_device = device\n        \n        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n        self.control_image_processor = VaeImageProcessor(\n            vae_scale_factor=self.vae_scale_factor, do_convert_rgb=True, do_normalize=False\n        )\n    \n\n    def load_models(self, core: Core, device: str, controlnet:Model, text_encoder: Model, unet: Model, vae_decoder: Model, vae_encoder: Model, state_dict, alpha_list):\n        if state_dict != None:\n            ov_unet = core.read_model(unet)\n            ov_text_encoder = core.read_model(text_encoder)\n            ##===Add lora weights===\n            visited = []\n            lora_dict = {}\n            lora_dict_list = []\n            LORA_PREFIX_UNET = \"lora_unet\"\n            LORA_PREFIX_TEXT_ENCODER = \"lora_te\"\n            flag = 0\n            manager = Manager()\n            for iter in range(len(state_dict)):\n                visited = []\n                for key in state_dict[iter]:\n                    if \".alpha\" in key or key in visited:\n                        continue\n                    if \"text\" in key:\n                        layer_infos = key.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\".\")[0]\n                        lora_dict = dict(name=layer_infos)\n                        lora_dict.update(type=\"text_encoder\")\n                    else:\n                        layer_infos = key.split(LORA_PREFIX_UNET + \"_\")[1].split('.')[0]\n                        lora_dict = dict(name=layer_infos)\n                        lora_dict.update(type=\"unet\")\n                    pair_keys = []\n                    if \"lora_down\" in key:\n                        pair_keys.append(key.replace(\"lora_down\", \"lora_up\"))\n                        pair_keys.append(key)\n                    else:\n                        pair_keys.append(key)\n                        pair_keys.append(key.replace(\"lora_up\", \"lora_down\"))\n\n                        # update weight\n                    if len(state_dict[iter][pair_keys[0]].shape) == 4:\n                        weight_up = state_dict[iter][pair_keys[0]].squeeze(3).squeeze(2).to(torch.float32)\n                        weight_down = state_dict[iter][pair_keys[1]].squeeze(3).squeeze(2).to(torch.float32)\n                        lora_weights = alpha_list[iter] * torch.mm(weight_up, weight_down).unsqueeze(2).unsqueeze(3)\n                        lora_dict.update(value=lora_weights)\n                    else:\n                        weight_up = state_dict[iter][pair_keys[0]].to(torch.float32)\n                        weight_down = state_dict[iter][pair_keys[1]].to(torch.float32)\n                        lora_weights = alpha_list[iter] * torch.mm(weight_up, weight_down)\n                        lora_dict.update(value=lora_weights)\n                    #check if this layer has been appended in lora_dict_list\n                    for ll in lora_dict_list:\n                        if ll[\"name\"] == lora_dict[\"name\"]:\n                            ll[\"value\"] += lora_dict[\"value\"] # all lora weights added together\n                            flag = 1\n                    if flag == 0:\n                        lora_dict_list.append(lora_dict)\n                    # update visited list\n                    for item in pair_keys:\n                        visited.append(item)\n                    flag = 0\n            manager.register_pass(InsertLoRA(lora_dict_list))\n            if (True in [('type','text_encoder') in l.items() for l in lora_dict_list]):\n                manager.run_passes(ov_text_encoder)\n            self.text_encoder = core.compile_model(ov_text_encoder, device)\n            manager.run_passes(ov_unet)\n            self.unet = core.compile_model(ov_unet, device)\n        else:\n            self.text_encoder = core.compile_model(text_encoder, device)\n            self.unet = core.compile_model(unet, device)\n\n        self.text_encoder_out = self.text_encoder.output(0)\n        self.controlnet = core.compile_model(controlnet, device)\n        self.unet_out = self.unet.output(0)\n        self.vae_decoder = core.compile_model(vae_decoder)\n        self.vae_decoder_out = self.vae_decoder.output(0)\n        self.vae_encoder = core.compile_model(vae_encoder)\n        self.vae_encoder_out = self.vae_encoder.output(0)\n    def prepare_image(self):\n        height = 512\n        width = 512\n        pad_height = 0\n        pad_width = 0\n        pad = ((0, 0), (0, pad_height), (0, pad_width), (0, 0))\n        return height, width, pad\n        \n    def prepare_control_image(\n        self,\n        image,\n        width,\n        height,\n        batch_size,\n        num_images_per_prompt,\n        device,\n        dtype,\n        do_classifier_free_guidance=False,\n        guess_mode=False,\n    ):\n        image = self.control_image_processor.preprocess(image, height=height, width=width).to(dtype=torch.float32)\n        image_batch_size = image.shape[0]\n\n        if image_batch_size == 1:\n            repeat_by = batch_size\n        else:\n            # image batch size is the same as prompt batch size\n            repeat_by = num_images_per_prompt\n\n        image = image.repeat_interleave(repeat_by, dim=0)\n\n        image = image.to(device=device, dtype=dtype)\n\n        if do_classifier_free_guidance and not guess_mode:\n            image = torch.cat([image] * 2)\n\n        return image\n    def prepare_mask_and_masked_image(self, image, mask, height, width, return_image=False):\n        if image is None:\n            raise ValueError(\"`image` input cannot be undefined.\")\n        \n        if mask is None:\n            raise ValueError(\"`mask_image` input cannot be undefined.\")\n        \n        # preprocess image\n        if isinstance(image, (Image.Image, np.ndarray)):\n            image = [image]\n        if isinstance(image, list) and isinstance(image[0], Image.Image):\n            # resize all images w.r.t passed height an width\n            image = [i.resize((width, height), resample=Image.LANCZOS) for i in image]\n            image = [np.array(i.convert(\"RGB\"))[None, :] for i in image]\n            image = np.concatenate(image, axis=0)\n        elif isinstance(image, list) and isinstance(image[0], np.ndarray):\n            image = np.concatenate([i[None, :] for i in image], axis=0)\n        \n        image = image.transpose(0, 3, 1, 2)\n        #image = image.astype(np.float32) / 127.5 - 1.0\n        image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n        \n        # preprocess mask\n        if isinstance(mask, (Image.Image, np.ndarray)):\n            mask = [mask]\n        \n        if isinstance(mask, list) and isinstance(mask[0], Image.Image):\n            mask = [i.resize((width, height), resample=Image.LANCZOS) for i in mask]\n            mask = np.concatenate([np.array(m.convert(\"L\"))[None, None, :] for m in mask], axis=0)\n            mask = mask.astype(np.float32) / 255.0\n        elif isinstance(mask, list) and isinstance(mask[0], np.ndarray):\n            mask = np.concatenate([m[None, None, :] for m in mask], axis=0)\n        \n        mask[mask < 0.5] = 0\n        mask[mask >= 0.5] = 1\n        mask = torch.from_numpy(mask)\n        \n        masked_image = image * (mask < 0.5)\n\n        # n.b. ensure backwards compatibility as old function does not return image\n        if return_image:\n            return mask, masked_image, image\n        \n        return mask, masked_image\n        \n    def prepare_mask_latents(\n        self, mask, masked_image, batch_size, height, width, dtype, do_classifier_free_guidance\n    ):\n        # resize the mask to latents shape as we concatenate the mask to the latents\n        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n        # and half precision\n        mask = torch.nn.functional.interpolate(\n            mask, size=(height // self.vae_scale_factor, width // self.vae_scale_factor)\n        )\n        mask = mask.to(device=g_device, dtype=dtype)\n\n        masked_image = masked_image.to(device=g_device, dtype=dtype)\n\n        # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n        if mask.shape[0] < batch_size:\n            if not batch_size % mask.shape[0] == 0:\n                raise ValueError(\n                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n                    f\" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number\"\n                    \" of masks that you pass is divisible by the total requested batch size.\"\n                )\n            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)\n\n        mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask\n        \n        mask = mask.to('cpu').detach().numpy()\n        return mask\n    \n    def _encode_vae_image(self, image):\n        image = image.to('cpu').detach().numpy()\n        image_latents = self.vae_encoder(image)[self.vae_encoder_out]\n        image_latents = 0.18215 * image_latents\n    \n        return image_latents\n\n    def __call__(\n        self,\n        prompt: Union[str, List[str]],\n        init_image: Image.Image,\n        mask_image: Image.Image,\n        control_image: torch.Tensor,#Image.Image,\n        num_inference_steps: int = 10,\n        negative_prompt: Union[str, List[str]] = None,\n        guidance_scale: float = 7.5,\n        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n        control_guidance_start: Union[float, List[float]] = [0.0], #single controlnet\n        control_guidance_end: Union[float, List[float]] = [1.0], #single controlnet\n        eta: float = 0.0,\n        latents: Optional[np.array] = None,\n        output_type: Optional[str] = \"pil\",\n    ):\n\n        # 1. Define call parameters\n        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n\n        do_classifier_free_guidance = guidance_scale > 1.0\n        # 2. Encode input prompt\n        text_embeddings = self._encode_prompt(prompt, negative_prompt=negative_prompt)\n\n        # 3. Preprocess image\n        height, width, pad = self.prepare_image()\n        orig_width = width\n        orig_height = height\n        dtype = torch.float32\n        num_images_per_prompt = 1\n        control_image = self.prepare_control_image(\n            image=control_image,\n            width=width,\n            height=height,\n            batch_size=batch_size * num_images_per_prompt,\n            num_images_per_prompt=num_images_per_prompt,\n            device=g_device,\n            dtype=dtype,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            guess_mode=False,\n        )\n\n        # 4. Preprocess mask and image - resizes image and mask w.r.t height and width\n        mask, masked_image, init_image = self.prepare_mask_and_masked_image(\n            init_image, mask_image, height, width, return_image=True\n        )\n\n        # 4. set timesteps\n        self.scheduler.set_timesteps(num_inference_steps)\n        timesteps = self.scheduler.timesteps\n        # at which timestep to set the initial noise (n.b. 50% if strength is 0.5)\n        latent_timestep = timesteps[:1].repeat(batch_size)\n        # create a boolean to check if the strength is set to 1. if so then initialise the latents with pure noise\n        strength = 1.0\n        is_strength_max = strength == 1.0\n        \n        # 6. Prepare latent variables\n        num_channels_latents = 4\n        return_image_latents = num_channels_latents == 4\n        latents_outputs = self.prepare_latents(\n            batch_size,\n            num_channels_latents,\n            height,\n            width,\n            text_embeddings.dtype,\n            latents,\n            image=init_image,\n            timestep=latent_timestep,\n            is_strength_max=is_strength_max,\n            return_noise=True,\n            return_image_latents=return_image_latents,\n        )\n        \n        if return_image_latents:\n            latents, noise, image_latents = latents_outputs\n        else:\n            latents, noise = latents_outputs\n        \n        text_embeddings_torch = torch.from_numpy(text_embeddings)\n        # 6. Prepare mask latent variables\n        mask = self.prepare_mask_latents(\n            mask,\n            masked_image,\n            batch_size,\n            height,\n            width,\n            text_embeddings_torch.dtype,\n            do_classifier_free_guidance,\n        )\n\n         # 6.1 Create tensor stating which controlnets to keep\n        controlnet_keep = []\n        for i in range(len(timesteps)):\n            keeps = [\n                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n                for s, e in zip(control_guidance_start, control_guidance_end)\n            ]\n            controlnet_keep.append(keeps[0]) #keeps[0] if isinstance(controlnet, ControlNetModel) else keeps)\n\n        # 7. Denoising loop\n        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                \n                #if isinstance(latents, (torch.Tensor)):\n                #    latents = latents.to('cpu').detach().numpy()\n                print(\"Denoising loop\\n\")\n                # Expand the latents if we are doing classifier free guidance.controlnet_pip\n                # The latents are expanded 3 times because for pix2pix the guidance\\\n                # is applied for both the text and the input image.\n                latent_model_input = np.concatenate(\n                    [latents] * 2) if do_classifier_free_guidance else latents\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n                #text_embeddings = np.split(text_embeddings, 2)[1] if do_classifier_free_guidance else text_embeddings\n\n                if isinstance(controlnet_keep[i], list):\n                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n                else:\n                    cond_scale = controlnet_conditioning_scale * controlnet_keep[i]\n                \n                result = self.controlnet([latent_model_input, t, text_embeddings, control_image.detach().numpy(), cond_scale])\n                down_and_mid_blok_samples = [sample * cond_scale for _, sample in result.items()]\n\n                # predict the noise residual\n                noise_pred = self.unet([latent_model_input, t, text_embeddings, *down_and_mid_blok_samples])[self.unet_out]\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = np.split(noise_pred,2) #noise_pred[0], noise_pred[1]\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(torch.from_numpy(noise_pred), t, torch.from_numpy(latents)).prev_sample.numpy()\n\n                # num_channels_unet == 4\n                init_latents_proper = image_latents[:1]\n                init_mask = mask[:1]\n                \n                if i < len(timesteps) - 1:\n                    noise_timestep = timesteps[i + 1]\n                    init_latents_proper = self.scheduler.add_noise(\n                        torch.from_numpy(init_latents_proper), torch.from_numpy(noise), torch.tensor([noise_timestep])\n                    ).to('cpu').detach().numpy()\n                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n\n                # update progress\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n        \n        if isinstance(latents, (torch.Tensor)):\n            latents = latents.to('cpu').detach().numpy()\n        # 8. Post-processing\n        image = self.decode_latents(latents, pad)\n\n        # 9. Convert to PIL\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n            image = [img.resize((orig_width, orig_height), Image.Resampling.LANCZOS) for img in image]\n        else:\n            image = [cv2.resize(img, (orig_width, orig_width))\n                     for img in image]\n\n        return image\n\n    def _encode_prompt(self, prompt:Union[str, List[str]], num_images_per_prompt:int = 1, do_classifier_free_guidance:bool = True, negative_prompt:Union[str, List[str]] = None):\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        # tokenize input prompts\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        text_input_ids = text_inputs.input_ids\n\n        text_embeddings = self.text_encoder(\n            text_input_ids)[self.text_encoder_out]\n\n        # duplicate text embeddings for each generation per prompt\n        if num_images_per_prompt != 1:\n            bs_embed, seq_len, _ = text_embeddings.shape\n            text_embeddings = np.tile(\n                text_embeddings, (1, num_images_per_prompt, 1))\n            text_embeddings = np.reshape(\n                text_embeddings, (bs_embed * num_images_per_prompt, seq_len, -1))\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            uncond_tokens: List[str]\n            max_length = text_input_ids.shape[-1]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            else:\n                uncond_tokens = negative_prompt\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"np\",\n            )\n\n            uncond_embeddings = self.text_encoder(uncond_input.input_ids)[self.text_encoder_out]\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = uncond_embeddings.shape[1]\n            uncond_embeddings = np.tile(uncond_embeddings, (1, num_images_per_prompt, 1))\n            uncond_embeddings = np.reshape(uncond_embeddings, (batch_size * num_images_per_prompt, seq_len, -1))\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = np.concatenate([uncond_embeddings, text_embeddings])\n\n        return text_embeddings\n\n    def prepare_latents(self, batch_size:int, num_channels_latents:int, height:int, width:int, dtype:np.dtype = np.float32, latents:np.ndarray = None,\n        image=None,\n        timestep=None,\n        is_strength_max=True,\n        return_noise=False,\n        return_image_latents=False,):\n\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if return_image_latents or (latents is None and not is_strength_max):\n            image_latents = self._encode_vae_image(image=image)\n        \n        if latents is None:\n            noise = randn_tensor(shape, dtype=dtype)\n            # if strength is 1. then initialise the latents to noise, else initial to image + noise\n            latents = noise if is_strength_max else self.scheduler.add_noise(image_latents, noise, timestep)\n            # if pure noise then scale the initial latents by the  Scheduler's init sigma\n            latents = latents * self.scheduler.init_noise_sigma if is_strength_max else latents\n        else:\n            noise = latents #.to(device)\n            latents = noise * self.scheduler.init_noise_sigma\n\n        outputs = (latents,)\n\n        if return_noise:\n            outputs += (noise,)\n\n        if return_image_latents:\n            outputs += (image_latents,)\n\n        return outputs\n\n    def decode_latents(self, latents:np.array, pad:Tuple[int]):\n\n        latents = 1 / 0.18215 * latents # 1 / self.vae.config.scaling_factor * latents\n        image = self.vae_decoder(latents)[self.vae_decoder_out]\n        (_, end_h), (_, end_w) = pad[1:3]\n        h, w = image.shape[2:]\n        unpad_h = h - end_h\n        unpad_w = w - end_w\n        image = image[:, :, :unpad_h, :unpad_w]\n        image = np.clip(image / 2 + 0.5, 0, 1)\n        image = np.transpose(image, (0, 2, 3, 1))\n        return image\n\ndef make_inpaint_condition(image, image_mask):\n    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n    print(type(image))\n    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n\n    assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n    image[image_mask > 0.5] = -1.0  # set as masked pixel\n    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n\n    image = torch.from_numpy(image)\n\n    #image = DiffusionPipeline.numpy_to_pil(image)\n    return image\n\ndef sd_inpaint(orignal_img, mask_img, prompt, negative_prompt):\n    args = parse_args()\n    controlnet = ControlNetModel.from_pretrained(\"./control_v11p_sd15_inpaint\", torch_dtype=torch.float32).cpu()\n    pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\"../../lora_SDXL/stable-diffusion-v1-5\", controlnet=controlnet)\n    \n    \n    tokenizer = CLIPTokenizer.from_pretrained('../../lora_SDXL/clip-vit-large-patch14')\n    scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    #scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n    \n    CONTROLNET_OV_PATH = \"controlnet-inpaint.xml\"\n    TEXT_ENCODER_OV_PATH = \"text_encoder.xml\"\n    UNET_OV_PATH = \"unet_controlnet.xml\"\n    VAE_DECODER_OV_PATH = \"vae_decoder.xml\"\n    VAE_ENCODER_OV_PATH = \"vae_encoder.xml\"\n    \n    core = Core()\n    #core.set_property({'CACHE_DIR': './cache'})\n    #====Add lora======\n    LORA_PATH = []\n    LORA_ALPHA = []\n    if args.lora_path != \"\":\n        LORA_PATH.append(args.lora_path)\n        LORA_ALPHA.append(args.alpha)\n        if args.lora_path2 != \"\":\n            LORA_PATH.append(args.lora_path2)\n            LORA_ALPHA.append(args.alpha)\n    \n    state_dict = []\n    # load LoRA weight from .safetensors\n    if len(LORA_PATH) == 0:\n        ov_pipe = OVContrlNetStableDiffusionPipeline(tokenizer, scheduler, core, CONTROLNET_OV_PATH, TEXT_ENCODER_OV_PATH, UNET_OV_PATH, VAE_DECODER_OV_PATH, VAE_ENCODER_OV_PATH, None, None, device=\"GPU\") #change to CPU or GPU\n    else:\n        [state_dict.append(load_file(p)) for p in LORA_PATH] #state_dict is list of lora list\n        ov_pipe = OVContrlNetStableDiffusionPipeline(tokenizer, scheduler, core, CONTROLNET_OV_PATH, TEXT_ENCODER_OV_PATH, UNET_OV_PATH, VAE_DECODER_OV_PATH, VAE_ENCODER_OV_PATH, state_dict, LORA_ALPHA, device=\"GPU\") #change to CPU or GPU\n    \n    init_image = load_image(\n        orignal_img\n    )\n    init_image = init_image.resize((512, 512))\n    mask_image = load_image(\n        mask_img\n    )\n    mask_image = mask_image.resize((512, 512))\n    \n    control_image = make_inpaint_condition(init_image, mask_image)\n    #prompt = [\"a handsome prince with a pink MLB hat\",\"a handsome man with ray-ban sunglasses\"]\n    #prompt = [\"a cute child with blue NFL hat\",\"a handsome man with ray-ban sunglasses\"]\n    \n    num_steps = 20\n    \n    #negative_prompt = [\"monochrome, lowres, bad anatomy, worst quality, low quality\",\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n    \n    np.random.seed(42)\n    start = time.time()\n    results = ov_pipe(prompt, init_image, mask_image, control_image, num_steps, negative_prompt)\n    end = time.time()-start\n    print(\"Inference time({}its): {} s\".format(num_steps,end))\n    \n    for i in range(len(results)):\n        results[i].save(\"../result\"+str(i)+\".png\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#!/usr/bin/env python3\n\nfrom roop import core\n\nif __name__ == '__main__':\n    core.run()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#!/usr/bin/env python3\n\nimport os\n\nfrom pipe_gpu_inpaint import sd_inpaint\n\ndef run_roop_with_ref(face_img):\n    os.system(\"python run.py -s \" + face_img + \" -t ../result0.png -o ..\\\\output0.png --execution-provider openvino\")\n    os.system(\"python run.py -s \" + face_img + \" -t ../result1.png -o ..\\\\output1.png --execution-provider openvino\")\n\ndef run_test(orignal_img, mask_img, face_img, prompt, negative_prompt):\n    sd_inpaint(orignal_img, mask_img, prompt, negative_prompt)\n    run_roop_with_ref(face_img)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#!/usr/bin/env python3\n\nfrom run_roop import run_test\n\norignal_img = \"../orignal.png\"\nmask_img = \"../mask.png\"\nface_img = \"../ref_face.png\"\n\nprompt = [\"a cute child with blue NFL hat\",\"a handsome man with ray-ban sunglasses\"]\nnegative_prompt = [\"monochrome, lowres, bad anatomy, worst quality, low quality\",\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n\nif __name__ == '__main__':\n    run_test(orignal_img, mask_img, face_img, prompt, negative_prompt)\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}